---
title: "Bayesian Inference, MCMC, and Genotyping Error: A Narrative Outline"
output: 
  html_notebook:
    toc: true
---

```{r results='hide', message=FALSE}
library(tidyverse)
```

## Preamble


In years past, Mike Miller has lectured on genotype likelihoods and genotype posteriors.
This year, however, we all got to hear from Mike about the coalescent, which is an incredibly
important topic in conservation genomics. So, I will spend a little more time talking
about genotype likelihoods and "probabilistic genotype calling."

But, I want to talk about these topics in the context of a little background
on Bayesian statistics and Markov chain Monte Carlo.

So, my plan for this session is to start with a simple exercise of estimating
an allele frequency in a Bayesian manner.  

First, we'll talk about estimating the allele frequencies alone:

1. Introduce acyclic directed graphs (DAGs)
1. Talk about proportionality to the joint probability (a nice perspective)
1. Show what inference of allele frequencies looks like if you 
   know the genotypes exactly. (We have a Shiny App for that!)
1. Talk about how we could learn about the posterior by simulating
   values from it.  Whether they were correlated (MCMC) or not (vanilla 
   Monte Carlo).


After that we will put another layer into the model:  

1. Add in the idea that genotypes are not observed directly, but rather
   underlie counts of reads of different alleles.
1. Show that computing the posterior distribution in this case
   is much more complicated.
1. Provide an intuitive explanation of how MCMC works in this case
1. Talk a little bit about what MCMC is
1. Let everyone explore genotype likelihoods and posteriors from
simulated sequencing read data in another Shiny App.

That will all be a bit theoretical/hyptothetical and rely on simulated data.
We will leave everyone with some tools and ideas that can be used to
investigate real data.

  
We will look at some published data sets and consider heterozygote
deficiencies in them, which will lead us to consider another
model---ones that use the observed genotype calls to estimate
a type of genotyping error that I call a "heterozygote miscall." In this
case we will

1. Motivate the problem by encountering some plots of expected vs observed
genotype frequencies in RAD seq data that are a little concerning.
1. Develop a simple model to estimate the overall heterozygote miscall rate
and see what that value is for a few data sets.
1. Extend the model to estimate het miscall rate as a function of read depth
and see that even the best of the data sets have moderately high
genotyping error at lower read depths.


After that, the remainder of the session will be spent letting people 
evaluate data sets that they might have laying around in VCF format,
making plots and estimating heterozygote miscall rates using my 
R package `whoa`.




## Bayesian Inference

### Personal Probability

To understand the Bayesian paradigm, it is useful to start with the awareness
that everyone has their own probabilities for events or outcomes, and these 
probabilities are informed by each person's experience, their prior beliefs,
the data that they have access to, and the inferential perspectives or _models_
that they can bring to the task of understanding and incorporating the information that they
have available.

Wait! What kind of crazy notion of probability is this?  Many of us might be 
familiar with the idea of probability being interpretable as the long-run average
frequency of outcomes if a particular event is repeated many times.  But this 
"personal probability" paradigm sounds quite different...

A good way to approach this is with some simple card play.  If I hand out
cards from a shuffled deck to five people in the room and tell them not to 
look at the cards, and then ask everyone what the probability is that the 
next card on the deck is the ace of spades, most everyone will have the 
same probability for that.  But if we let the 5 people look at their cards 
and then ask each, in turn, what they now think is the probability that the
top card is the ace of spades, they will likely update their beliefs based
on the card that they possess, as well as the responses of any cardholders
before them.

There a lot of assumptions that go into these calculations.  What are they?

At any rate, it is easy to see that people have different probabilities for the
top card being an ace of spades depending on how much information they have.

### Probability as a "Measuring Stick" for Uncertainty

A big switch being made here is that we are now using probability as a way 
of expressing or measuring each person's "uncertainty" about an event.  This
is quite a departure from thinking about probability only as a property that
can be discussed in connection with events that we can envision happening
multiple time (so that we might derive a notion of probability as the average
frequency of occurrence over very many trials.)

This interpretation of probability is considerably more flexible than the
"frequentist" interpretation, and it is also already familiar to most of us:
we are already quite comfortable with being told that the probability it is 
going to rain today is 30%, even though there will only ever be one today. With 
probability as a measure of uncertainty, you can use probabilities to talk about
things that will only happen once. What is the probability that:

- there will be three more category 5 hurricanes this season?
- Joe Biden will win the popular vote but not the Electoral College
in November?

Not only that, but now we can use probability to make statements about how
uncertain we are about things that are deterministic.  For example:

* What is your probability that Monrovia is the capital of Liberia? 

So, if we are trying to measure:

- height, then we use centimeters,
- mass, then we use kilograms,
- electrical resistance, we use Ohms
- uncertainty?...we use probability...

### An interesting digression

Probability as a field of mathematical study did not necessarily "grow up" with the
intent to be a way to measure uncertainty.  (It partly grew out of contemplation
of "Borel subsets" and many much more esoteric concepts.) But the way that
it was developed as a mathematical construct turns out to be precisely what 
is required if we desired to extend the system of logic in which 1 means
"absolutely true" and 0 means "absolutely false" to a logical system in which
there could be degrees of uncertainly between 0 and 1.


### Updating Beliefs

A characteristic feature that we saw playing our card game is that when new
information arrives, people update their beliefs, and hence they update
their degree of undertainty (their probabilites...).

Clearly, if we want to be Bayesians and use probability to express uncertainty
about things, we need a coherent, reasonable way to update our beliefs.  This is
done using basic principles from probability, which were described (for inferential
purposes) by the Reverend Thomas Bayes (born 1702), and is hence known as 
Bayes' Law, or Bayes Theorem.  

Bayes Theorem is typically expressed in an equation that probably feels
pretty opaque to most people (it certainly did to me when I was a young
biology graduate student.)  I remembered seeing it something like this:
suppose that you have observed some data $D$, and that you have $k$ different
hypotheses for its occurrence, and for each hypothesis, $i$,  you can compute
$P(D|H_i)$ the probability of observing D given that hypothesis $i$ is true.

The posterior probability of a certain hypothesis $H_i$ given $D$ is then:
$$
P(H_i|D) = \frac{P(D|H_i)P(H_i)}{\sum_{j = 1}^k P(D|H_j)P(H_j)}
$$
Yack!  Let's see if we can find an easier way to talk about this!

We will (at least I hope it will be easier) and we will do so while talking
about a simple problem that we can build upon to make more complex models:
estimating the frequency of an allele in a population.

### A Simple Conditional Probability and DAGs

Almost all of the probabilities that we talk about routinely are
_conditional probabilities_.  That is, they are probabilities that are
expressed conditional on some other variable or thing taking a certain value. 

Here we will start beating to death an example that involves genotype and 
allele frequencies, but bear with us, because we will end up using what we
learn from this example---and building upon it---throughout this
lecture. 

If we ask about the probability that an individual is homozygous
for the $A$ allele, we might first have to make an assumption that the population
is in Hardy-Weinberg equilibrium, and then we also need to assess this probability
_conditional on_ the frequency of the $A$ allele being $p_A$ in the population.

So, we could write:
$$
P(\mbox{Individual is AA}~|~p_A) = p_A^2
$$
If we knew the value of $p_A$, then it would be easy to do this calculation.  This is
an example of probability "running forward".  We will see later that when
we want to do inference we need to "run probability backward" (which is what 
Bayes' Theorem lets us do.) 

Recall that HWE means that the allelic type of the two gene copies carried in 
an individual are conditionally independent given the allele frequency.  Hence, 
the probability of drawing the first gene copy as an A is $p_A$, and the probability
that the second gene copy is drawn as an A (given that the first was an $A$), is also $p_A$.
So the probability that the individual is $AA$ is the product of those: $p_A^2$.

We can draw a diagram of that called an acyclic directed graph (or DAG).  These are
very helpful for visualizing models as they become more complex.  

![](diagrams/simple1.png)

The key thing to know about DAGs is that they express the factorization of 
the joint probability of the variables into a product of conditional
probabilities. (Specifically a product over nodes of the probability of
each node conditional on its parents.)  

From the way 
the DAG is drawn above, it is clear that we can say:
$$
P(\mbox{Individual is AA}~|~p_A) = P(Y_1 = A~|~p_A) P(Y_2 = A~|~p_A) = p_A \cdot p_A =  p_A^2
$$

Here is a quick summary of DAGs:

![](slide_grabs/dag-notes.png)



### A Picture of Inference of Allele Frequencies From a Sample of Genotypes

The previous discussion assumed that the allele frequency was known, and we wanted
to compute the probability of an individual's (unknown) genotype.  For that exercise,
we were running from an observed node in the graph ($p_A$) to two unobserved ones.  This 
is a straightforward calculation of probabilities.

Much of the time in science, we are able to observe the variables at the end of the 
arrows in a DAG, and we want to make inferences about the variables that are "upstream"
of them in a DAG. For an example, let us consider the situation in which we sample $N$
diploids and observe their genotypes, and from that we want to infer the allele frequency
$p_A$.  

The DAG for this situation looks like this:

![](diagrams/infer1.png)

From which it is immediately apparent that the probability of all the 
data, say $\boldsymbol{Y} = (Y_{1,1}, Y_{1,2}, Y_{2,1}, \ldots, Y_{N,1}, Y_{N,2})$
is just a simple product of Bernoulli random variables:
$$
P(\boldsymbol{Y}~|~p_A) = \prod_{i=1}^N P(Y_{i,1}~|~p_A)P(Y_{i,2}~|~p_A) 
$$
In fact, if we define each $Y$ to be 0 or 1 as follows:
$$
\begin{align}
Y = 0 & ~~~~~\mbox{with probability} ~~~ 1 - p_A & ~~\mbox{(i.e., it's an}~a) \\
Y = 1 & ~~~~~\mbox{with probability} ~~~ p_A & ~~\mbox{(i.e., it's an}~A)
\end{align}
$$
Then it is not too hard to see that
$$
P(\boldsymbol{Y}~|~p_A) = p_A^{\sum Y_{i,j}} (1- p_A)^{2N - \sum Y_{i,j}}
$$
This is a probability function.  But if you consider this as a function of $p_A$ with 
$\boldsymbol{Y}$ considered as fixed, then it is often referred to as the 
_likelihood function_.


### Methods of Inference

We see that we are trying to learn something (make inference) about 
$p_A$ (an unshaded/unobserved variable) that is upstream of our observed
data.  This is inference.  

To put some concrete numbers on this.  Let's say that $N = 100$ diploids, and 
out of the 200 gene copies, 73 of them were $A$ alleles.

There are lots of ways that you might do inference.  Here are a few:

- _Method of the Eyeball_: Look at your sample and surmise that the fraction of A alleles
in the sample is a good estimate of the fraction of $A$ alleles in the population.
- _Method of Moments_: This formalizes the "Method of the Eyeball" by equating the sample mean
with the population mean.
- _Method of Maximum Likelihood_: Find the value of $p_A$ that maximizes the probability of
observing your sample.  In other words, consider the probability
$P(\boldsymbol{Y}~|~p_A)$ as a function of $p_A$, where the data,
$\boldsymbol{Y}$, are fixed, and then find the value of $p_A$ that maximises that _likelihood_ 
function.

All of those methods give you a _point estimate_ for $p_A$.  An alternative to
these methods is to be Bayesian and find the _posterior distribution_ for $p_A$
conditional on the data, $\boldsymbol{Y}$. 

Before we do this, we are going to want to review a view simple facts about
marginal, conditional, and joint probabilities.

### Some important probability rules

If you have two different events $A$ (no relation, necessarily, to the big-A allele ) and $B$,
and we use $A$ and $B$ to refer to the outcome of each, then here are some things
that are always true:

- $P(A)$ and $P(B)$ are referred to as marginal probabilities.
- The joint probability of $A$ and $B$ is the probability that those two outcomes
occurred, and it can be computed as the product of a marginal probability and a conditional
probability, in two different ways:
$$
P(A,B) = P(A)P(B~|~A) = P(B)P(A~|~B)
$$
- This means that conditional probabilities can be computed from the joint probability:
$$
P(A~|~B) = \frac{P(A,B)}{P(B)}~~~~~~~~~~~~~~~~~~~~~~~~
P(B~|~A) = \frac{P(A,B)}{P(A)}
$$

And that leads us to an expression for Bayes Theorem that I find easier to grok out:
$$
P(A~|~B) \propto P(A, B)
$$
where we are thinking of $P(A|B)$ as a function of $A$ with $B$ fixed.  It is typically
easy to compute the joint probability, $P(A,B)$, and then you just have to remember that
$P(A~|~B)$, since it is a probability on $A$, must sum to one over all possible values of $A$.

The same is true for:
$$
P(B~|~A) \propto P(A, B)
$$

### Bayesian Inference for $p_A$

Back to our simple example.  If we want to do Bayesian inference for $p_A$ we 
see that we will want to compute the _posterior probability_:
$$
P(p_A|\boldsymbol{Y})
$$
which  we now know is going to be proportional to the _joint probability_:
$$
P(p_A,\boldsymbol{Y})
$$
and what we currently have at our disposal is the _likelihood_:
$$
P(\boldsymbol{Y}~|~p_A)
$$
We could get the joint probability by using the likelihood in the product:
$$
P(p_A,\boldsymbol{Y}) = P(\boldsymbol{Y}~|~p_A) P(p_A)
$$
But what is this $P(p_A)$?!

It is the _prior distribution_ for $p_A$.  It is a
necessary ingredient to be able to use the likelihood to compute the
joint probability (and, hence, the posterior probability), and we envision
it as a probability distribution that
expresses our degree of belief about $p_A$ _before we look at the data_.

Note that this all boils down verbally to: **The posterior distribution is 
proportional to the prior times the likelihood**. 


### A family of priors for $p_A$

Since $p_A$ is a proportion, an obvious choice for prior would
be a beta distribution.  The beta distribution gives a continuous prior
distribution on a value that is between 0 and 1.  It has two parameters,
often called $\alpha_1$ and $\alpha_2$.  Here are some examples:
```{r, echo=FALSE, message=FALSE}
library(tidyverse)
xs <- seq(0,1, by = 0.001)
atib <- list(
  `a1 = 1/10;  a2 = 1/10` = tibble(p_A = xs, density = dbeta(xs, 0.1, 0.1)),
  `a1 = 1/2;  a2 = 1/2` = tibble(p_A = xs, density = dbeta(xs, 0.5, 0.5)),
  `a1 = 1;  a2 = 1` = tibble(p_A = xs, density = dbeta(xs, 1, 1)),
  `a1 = 5;  a2 = 5` = tibble(p_A = xs, density = dbeta(xs, 5, 5)),
  `a1 = 10;  a2 = 2` = tibble(p_A = xs, density = dbeta(xs, 10, 2))
) %>%
  bind_rows(.id = "parameters")

g <- ggplot(atib, aes(x = p_A, y = density, colour = parameters)) +
  geom_line() +
  ylim(0, 4.3)


ggsave(g, filename = "figures/beta_densities.pdf", width = 7, height = 5)

g
```

The beta density for a random variable $X$ has the form:
$$
p(x | \alpha_1, \alpha_2) = \frac{\Gamma(\alpha_1 + \alpha_2)}{\Gamma(\alpha_1)\Gamma(\alpha_2)}
x^{\alpha_1 - 1}(1-x)^{\alpha_2 - 1}
$$
The part that looks hairy is a few Gamma functions.  Don't worry about those---it
is a constant.  The important part (the "kernel", as they say...) is:
$$
x^{\alpha_1 - 1}(1-x)^{\alpha_2 - 1}
$$
Or, if we wanted this to a be a prior on $p_A$, the prior would be proportional to:
$$
{p_A}^{\alpha_1 - 1}(1-p_A)^{\alpha_2 - 1}
$$
And, if we wanted to be even more specific, we could choose $\alpha_1 = \alpha_2 = 1$ to
give ourselves a uniform prior which is proportional to 1:
$$
P(p_A) \propto {p_A}^{1 - 1}(1-p_A)^{1 - 1} = {p_A}^{0}(1-p_A)^{0} = 1
$$


### A Graphical Aside

Just wanting to point out that if you want to be a Bayesian, you 
can't have any unobserved nodes on your DAG that don't have any parents.
Those priors that you use/accept/assume are always the uppermost nodes
in a DAG.  I usually use gray nodes to denote them. For example, here is the
DAG for the Bayesian version of the previously shown DAG:

![](diagrams/infer1-with-prior.png)



### The posterior for $p_A$

Now that we have a choice for our prior, $P(p_A)$, we can easily find something that
is proportional to the posterior distribution.  What is proportional to the posterior
distribution? Remember, **The posterior distribution is 
proportional to the prior times the likelihood** (which, don't forget, is the
_joint_ probability).  Remember that the likelihood looks like:
$$
P(\boldsymbol{Y}~|~p_A) = p_A^{\sum Y_{i,j}} (1- p_A)^{2N - \sum Y_{i,j}}
$$
And, in our example, the number of $A$ alleles is 73 (out of 200), so
$\sum Y_{i,j} = 73$ and $2N - \sum Y_{i,j} =  127$.

Since we are only going to worry about finding something that the
posterior probability is proportional to, we can drop any constants and 
we get this for the prior times the likelihood:
$$
\begin{align}
P(p_A~|~\boldsymbol{Y}) &\propto {p_A}^{0}(1-p_A)^{0} \times p_A^{73}(1-p_A)^{127} \\
  & \propto p_A^{73}(1-p_A)^{127} \\
  & \propto p_A^{74 - 1}(1-p_A)^{128 - 1} \\
\end{align}
$$
which is precisely the kernel of a beta distribution with parameters 
74 and 128.  

Aha! So, the posterior here is a beta distribution with $\alpha_1 = 74$
and $\alpha_2 = 128$.  We can plot that if we want:
```{r, echo=FALSE, message=FALSE}
xs <- seq(0, 1, by = 0.001)
btib <- tibble(p_A = xs,
               density = dbeta(xs, 74, 128))
g <- ggplot(btib, aes(x = p_A, y = density)) +
  geom_line() +
  ggtitle("Posterior Distribution of p_A")


ggsave(g, filename = "figures/pa_posterior.pdf", width = 7, height = 5)

g
```

In fact, we can do one better than just plotting that.  I've prepared a Shiny Notebook
that lets you play with different values of the allele frequency ($p_T$ in this case---the
frequency of the $T$ base at a genome position), simulate diploid genotype data from that,
and then compute the posterior distribution, whilst fiddling with the prior parameters,
if desired.

The one downside of Shiny apps is that they don't run properly on the RStudio Server that has
been set up for ConGen.  Nor will they run properly in the Remote Desktop version of the same.

But this is not a huge issue, since you should be able to run it on your own computer
with RStudio.  The procedure for that is:

1. Go to the Eric Anderson folder on the ConGen Box drive: [https://umt.box.com/s/jdsq97eg2tsse3mw4ig5f3h2ynmdhg74](https://umt.box.com/s/jdsq97eg2tsse3mw4ig5f3h2ynmdhg74)
2. Right click (on a mac Cntrl-click) the top folder called `ngs-genotype-models`, and choose "Download" from the pop-up menu.  (On some browsers, that fails.  In which case, click `ngs-genotype-models` and then from within that folder on Box choose the "Download" button at
the upper right)
3. That downloads the whole folder as a zip archive.  Unzip the whole thing to a directory called `ngs-genotype-models` on your computer.
4. Go inside that folder and double click `ngs-genotype-models.Rproj` to open the RStudio project.
5. Within RStudio, open `001-allele-freq-estimation.Rmd`
6. Install any packages RStudio says you might need.
7. Then choose "Run Document" at the top of the source editor window.

**If that does not work for some of the students**: you can go directly to the website:
[https://eriqande.shinyapps.io/ngs-genotype-models/001-allele-freq-estimation.Rmd](https://eriqande.shinyapps.io/ngs-genotype-models/001-allele-freq-estimation.Rmd).  But, be warned that if too many people use that link it will overwhelm my free ShinyApps account.  

Now, I am going to break students into break-out groups of 4 or 5 (randomly) for about 15 minutes.

Take a few minutes to introduce yourselves to one another, then play with the Shiny app and talk to one another about it as you do.  Maybe even work togethery to do these things:

* Input beta distribution parameters to get these different shapes:
    - an upward facing U
    - a flat line
    - a curve that keeps going up at one of the boundaries
    - a hill with a peak at 0.3
* Observe how the posterior distribution changes when sample size changes. 
* With a true allele frequency of $p_T = 0$, with a sample size of 50 diploids, find
values of the prior parameters that will give you a posterior centered on 0.5 (in other words
a ridiculously heavy prior...)

I'll try to say hello in the different break out rooms.



### To the Bayesian, the posterior distribution is everything

To the Bayesian, the posterior distribution contains all the information
that the data has about the parameter to be estimated---in this case, $p_A$.
And, the full posterior distribution, unsummarized, expresses that all that information
as fully as can be done.  

However, sometimes you don't want to carry around a full posterior distribution, 
especially if it is a distribution on multiple variables (multidimensional).

Often you will want to summarise the posterior distribution.

### Some standard ways of summarizing the posterior

1. The posterior mean.  Gives a point estimate that minimizes squared error posterior loss.
2. The posterior median.  Gives a point estimate that minimizes absolute posterior loss.
3. The posterior mode. Gives a point estimate that minimizes 0-1 posterior loss.
4. The Bayesian Credible Interval: provides an interval estimator. Like a confidence interval, but infinitely easier to interpret correctly...




## Monte Carlo sampling from a posterior distribution

In the above segment, we obtained a posterior distribution analytically.  That
is not the way things typically work out.  It is more common that
one is not able to obtain the posterior analytically, but, rather, can only
obtain a _sample_ from the posterior distribution.  With a sample from the posterior,
however, it is possible to use Monte Carlo to approximate virtually any quantity
that might be of interest.  Let's look at that with our simple example.

### A "vanilla" Monte Carlo sample from the posterior for $p_A$ 

Even though we can compute the posterior analytically, it can still
be convenient to obtain a sample from it.  We can sample 1 million values
from the posterior for $p_A$ like this:
```{r}
pA <- rbeta(n = 10^6, 74, 128)
```
The first 10 values of that sample look like:
```{r}
pA[1:10]
```

We call this a "vanilla" Monte Carlo sample because every member of the sample
was independent---this is not _Markov chain_ Monte Carlo.

If we wanted to use the sample to approximate the full posterior, we could do 
that with a histogram:
```{r}
hist(pA, breaks = 100)
```

If we want the posterior mean, that is easy, too:
```{r}
mean(pA)
```
as is the posterior median:
```{r}
median(pA)
```
Or the standard deviation of the posterior distribution:
```{r}
sd(pA)
```
Or, the 90%-equal-tail Credible Interval
```{r}
quantile(pA, probs = c(0.05, 0.95))
```
All of those quantities could have been obtained analytically in this case,
but it is a lot simpler to just work with a Monte Carlo sample because the 
operations are the same with every Monte Carlo sample (which is not necessarily
true of every analytical distribution...).

### Transformations are simple with a Monte Carlo sample

Imagine for a moment that what you really wanted to estimate was the 
probability that, if you sampled three individuals, in sequence, from the population,
the first one would be homozygous $AA$, the second one would be heterozygous $Aa$
and the third one would be homozygous $aa$. Since we are being Bayesians here, 
we will want to compute the posterior distribution of that probability given the 
observed data.  This is something that would be quite messy to do analytically,
but it is easy to use the Monte Carlo sample to do it.  Clearly, for a given $p_A$, 
the probability of the sequence ($AA$, $Aa$, $aa$) is
$p_A^2\times 2 p_A (1-p_A) \times (1-p)^2$.  So, the posterior distribution of that
quantity is simply: 
```{r}
three_geno_seq_posterior <- pA^2 * 2 * pA * (1 - pA) * (1 - pA)^2
```
And we can look at it:
```{r}
hist(three_geno_seq_posterior, breaks = 100)
```

## Probabilistic Genotype Calling / Allele frequency estimation {#probcall}

We are now going to expand our allele frequency model just a little bit, to get a taste for 
how we might infer both allele frequencies and genotypes given short read data.

Let's consider this setup: we are trying to infer genotypes in individuals and estimate
the allele frequencies in a population at a SNP in which the variants are the two
different bases,  $C$ or $T$.  We know where the SNP occurs (we are disregarding the problem
of inferring whether a SNP is there) but we do not know its frequency in the population.
We have sequenced $N$ individuals at this SNP.  We don't know the genotype of any of these
individuals, but we get to observe the number of sequencing reads on each individual, $i$,
that contain a $C$, and the number of reads that contain a $T$ at the site.  We call that
observed variable $\boldsymbol{R}_i = (R_{i,C}, R_{i,T})$, a two-vector.  We include in our model 
a sequencing error rate $\mu$ which, for simplicity here, we will assume is known.
With probability $\mu$ a read from a chromosome that really contains a $C$ at the site will
report a $T$, while with probability $1 - \mu$ the base at the site is correctly reported as a $C$.  Similarly for chromosomes that contain a $T$ at the site, with probability $\mu$ a read
will report a $C$ and with probability $1 - \mu$ it will correctly report a $T$.  

Here is a DAG for the model:

![](diagrams/genos-and-reads-dag.png)

  
  
All right.  This is fun.  Now there are _two_ layers of unknown variables (the $Y$'s and $p_C$ 
that we will try to infer together, at the same time).

### Why is this a good idea?

Because the allele frequency estimate is there to inform the inference of genotypes.

If the frequency of the $C$ allele is quite low, that will provide a lot of
evidence in the model against inferring someone to be homozygous for the $C$ allele.

And this would be a good thing, because after a little bit of looking at published
data sets of genotypes called by traditional means (i.e. apparently without
considering allele frequencies), it appears there is a bit of an epidemic
of genotyping error in RAD sequenced data.


### Conditional probabilities in the model

Most of these we have seen before:

- $P(p_C | \alpha_C, \alpha_T)$ is just a beta prior on $p_C$.
- $Y_{i,1}$ is a Bernoulli trial with success probability (i.e. probability of
getting a $C$ of $p_C$.)
- The only new one is the probability distribution of read counts given the genotype and
the genotyping error rate: $P(\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \mu)$

### The conditional probability $P(\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \mu)$

This thing turns out to be pretty easy.  It can be broken down into cases:

1. **$i$ is homozygous for $C$ (i.e., $Y_{i,1} = Y_{i,2} = C$):** In this case,
the only way to get a read with a $T$ at the site is by a sequencing error, which 
happens with probability $\mu$.  Thus, it is akin to drawing $M = R_{i,C} + R_{i,T}$
balls out of an urn (with replacement), a fraction $\mu$ of which
say $T$ on them, the rest which say
$C$ on them.  In such a case $R_{i,C}$ is binomially distributed, $M$ draws with
success probability $1-\mu$.
2. **$i$ is homozygous for $T$ (i.e., $Y_{i,1} = Y_{i,2} = T$):** Same argument
as above, $R_{i,C}$ is binomially distributed, $M$ draws with
success probability $\mu$ this time, rather than $\mu$.
3. **$i$ is homozygous for $T$ (i.e., $(Y_{i,1} = C, Y_{i,2} = T)$
or $(Y_{i,1} = T, Y_{i,2} = C)$):**  Because we have assumed sequencing error rates
are symmetrical between different bases, this is easily figured out---the reads are
like balls you are sampling (with replacement) from an Urn in which half of the balls 
are $T$'s and half of them are $C$'s.

Note that when this probability is considered a function of the genotype, (with the
observed reads treated as fixed), it is know as the _genotype likelihood_:
$$
L(Y_{i,1}, Y_{i,2}; \boldsymbol{R}_i, \mu) = P(\boldsymbol{R}_i~|~Y_{i,1}, Y_{i,2}, \mu)
$$

An easier way to see this might be a picture:

![](figures/genotypes_and_reads.svg)

_Genotype likelihoods_ are measures of how much support the _read_ data gives
to the three possible genotypes ($CC$, $CT$, or $TT$, in this case).

They are a critical ingredient for using read information for doing
population genomics, but they are only part of the story.

### Some of the assumptions in the above model...

There are quite a few but the biggest is that each read is independent of the others.
That assumption would be violated if:

- the sequences upon which the $C$ (or the $T$) occurred were somehow subject to
greater amplification during any PCR steps, or were more likely to attach to
"the lawn" on an Illumina machine.
- there are PCR duplicates.  This can be a really big issue for RAD methods that
don't have a good mechanism for filtering PCR duplicates.



### An intuitive picture of how Monte Carlo might proceed

The unobserved genotypes of the individuals in the above model are what are sometimes
called "missing data".  They are not missing data in the sense of small holes in your
data set that resulted from some data-collection failure.  They are missing in the sense
of "What is missing here is something that would make this whole problem easier".  Clearly,
as we showed in an earlier section, if we knew these genotypes exactly, doing Bayesian
inference for $p_C$ is pretty darn easy.  

Another name for this "missing data" is "latent variables."  Many formulations of MCMC 
are developed with these types of latent variables.  In our case, a brief sketch of how 
MCMC would proceed starts with a bit of wishful thinking.  We say, "Wouldn't it be great if
we actually knew the value of $p_C$...Hey! We don't, but let's just start with a guess and 
pretend it is true."  So, we would start by initializing the model and setting $p_C$ to some 
random value, let us call that value $p_C^{(0)}$.

Now, if we know the value of $p_C$, then the only thing that would be left unknown in the model
would be the genotypes of the individuals.  But looking at the graph it is clear that they 
are all conditionally indendent given $p_C^{(0)}$ and the observed read depths.  So, 
pretending that $p_C^{(0)}$ is the truth, we could compute, for each individual $i$, the 
posterior probabilities of $i$'s genotype (the three cases, 1--3 above). (By the way, this 
distribution is often called the "full conditional distribution").  

Then we would simulate values of the genotypes from those posterior distributions.  Aha!
once we have those simulated, we can pretend that they are real and use them to 
come up with a new estimate of $p_C$ by computing the posterior distribution (full conditional)
for $p_C$ given the current values of the genotypes and then simulating a value from
that distribution.  Call that $p_C^{(1)}$, and then go back to simulating new values for the
genotypes.  

Doing that over and over again creates a Markov chain that does a random walk 
over the space of all possible genotypes of the $i$ individuals and the unknown
allele frequency $p_C$, with the amount of time the chain spends in any state being 
_proportional to the posterior probability of that state!_.

This is actually Markov chain Monte Carlo.


### What is the Markov Chain part of MCMC?

We've seen that Monte Carlo in this Bayesian context is all about basing
one's inference on a _sample_ from the posterior distribution.  However, it turns
out that you often can't obtain _independent_ samples from a complex posterior distribution.
In high dimensions
that is just not possible.  But, it _is_ possible to construct an ergodic Markov chain that
has as its limiting distribution the posterior distribution you want to be sampling from. In
fact, that is what our "wishful thinking" procedure with the "missing data", described
above, is actually doing.

What?!

OK, we will talk briefly about Markov chains.  Basically a Markov chain is a
stochastic process (like a random walk) in which the probability of ending up 
in some place after the next step depends only on where it currently is---not
on how it got there...

A classic example is a random walk, we will consider one with scattering boundaries:

![](slide_grabs/random-walk.png)

The important take-home message from the `tpm` Computer Demo is that if you run a Markov chain
long enough, it spends time in different states with a predictable pattern called the 
limiting distribution.  In other words, you can take a sample of the states visited by the
chain and use that as a Monte Carlo sample.  That is Markov chain Monte Carlo.


### Why MCMC is so great for Bayesians

Via MCMC, it is possible to obtain samples from the posterior distribution
even if the posterior distribution is highly-multidimensional and complicated.

There are two main reasons for this:

1. A Markov chain can be devised to sample from the posterior distribution
without knowing the normalizing constant.  In other words, if you can compute the
joint probability (prior times likelihood) for a fully-observed version of your model,
that is all you need. (Note that you can pretty much always compute the joint probability 
for a fully-observed model.)
2. The Markov chain so devised can be constructed by proposing changes only to small
parts of the model, which makes it possible to break down complex distributions in
ways that make it manageable to sample from them.

The details of this are beyond the scope of a 1.25 hour lecture.

### Why MCMC can be not-so-great

1. It can be computationally expensive and take a long time.
2. It can mix poorly, i.e. it might not sample reliably from all areas
of the distribution in proportion to (or even approximately in proportion to) their
posterior probability.
3. It can appear to be mixing well, but actually not be mixing well.  It is important to
perform multiple runs from different starting values to assess convergence.
4. Especially in "canned packages" it can let you do "black-boxy" inference in models that are 
so complicated that it is hard to have proper intuition about how they behave or perform given
the vararies of your own data.

Again, these topics are beyond the scope of this presentation.  But grab me at some point if you want to talk about it, or consider the MCMC course at the [Summer Institute in Statistical Genetics](https://si.biostat.washington.edu/suminst/sisg).


### Playing with genotype likelihoods and genotype posteriors

The genotype likelihoods can be calculated from the read data alone.

But, if we combine that with allele frequencies in our model:

![](diagrams/genos-and-reads-dag.png)

...then we can also compute the posterior probability (by MCMC sampling)
of each genotype.  If you are sampling from a single population, then this
can provide a better estimate of the true genotype.  

### Shiny Interlude #2

From the RStudio project that you downloaded previously:

- Open `002-genotype-likelihoods-from-reads.Rmd`
- Install any packages that RStudio tells you that you might need. (i.e. `install.packages("cowplot")` if need be...)
- Hit the "Run Document" Button

(If this doesn't work for you, then you can access the Shiny app over the web:
[https://eriqande.shinyapps.io/ngs-genotype-models/002-genotype-likelihoods-from-reads.Rmd](https://eriqande.shinyapps.io/ngs-genotype-models/002-genotype-likelihoods-from-reads.Rmd)
)

Then you can simulate genotypes, as before, but now, you can
simulate reads from those genotypes, and then compute genotype posteriors using
MCMC.


### Back to breakout rooms...some questions:

1. What are the likelihoods for the three different possible genotypes when the read depth is 0 for an individual?
2. What does it take for the likelihood to be highest for the heterozygote hypothesis?
3. Is it more likely that a true heterozygote genotype will have a posterior probability that is highest for the hyothesis of "common homozygote" or the hypotheses of "rare homozygote."
4. How does the posterior distribution of the allele frequency computed from read data (the transparent blue histogram) compare to the posterior distribution if you know the genotypes exactly (the black line)?  How does this change when read depth is increased or decreased?
5. If you only have a single read from a heterozygous individual, will that individual's maximum likelihood genotype ever be "heterozygote."  What about its maximum _a-posteriori_ genotype? What are the conditions that lead to the heterozygous individual with only a single read having a high posterior probability of being a heterozygote?
6. When read depths are low, even if you are calling genotypes using the highest posterior probability, do you expect the results to be very accurate?




## Estimating Genotyping Error Rates in some RAD datasets

That was a lot of fun, but it was entirely playing with "made-up,"
simulated data to explore genotype likelihoods and posteriors.

What might we do to look directly at data that we have, and assess
it?

**It is important to recognize in this section that we are dealing only with
_called_ genotypes and we are not considering genotype likelihood information.
Effectively we are saying, "If we have called genotype data, what is one way we look at
it to assess its quality?"**

### Looking at Willow Flycatchers

Below is a plot I made of
data from Willow Flycatchers (Ruegg et al. 2018, _Ecol. Lett._)  They
called 105,000 SNPs in almost 200 individuals, and shown here are 47 individuals from the
imperiled population of the _extimus_ subspecies in Arizona. It is a plot of 
the expected frequency of each genotype  (0 = homozygous for the reference allele, 
1 = heterozygote, 2 = homozygous for the alternate allele) on the $x$-axis against the 
observed frequency of each genotype, across all loci polymorphic amongst those 47
individuals.

![](figures/wifl_big_pop_no_bounds.png)

These data were obtained by alignment to a de-novo assembled genome with SNPs called
using GATK, with light filtering (read depths > 6 or 10, and Phred-scaled
genotype likelihoods > 30).  

(These were called directly from the genotype likelihoods, but had high depth).

Note numbers up there: 

- **n** is the number of individuals in the population.
- **n_f** is the number left after tossing out anyone with more than 90% missing data.
- **L** is the number of loci in the whole data set.
- **L_f** is the number of loci remaining after removing those that are monomorphic in the
focal population, or which are missing data from more than 70% of the individuals.



It looks reasonably good---you want the observed to fall along the expected line---apart
from some obvious outliers that are all heterozygotes: clearly more than one genomic region
aligning to the same spot in the assembled genome.

### What should these plots look like?

It is easy to see what these plots should look like under Hardy-Weinberg equilibrium
by simulating it.  Here are results of simulations with sample sizes of 
20 and 40 individuals.

![](figures/simulated-nice-data.png)

Also, if you do a little thinking and some algebra you can see that, because
the expected genotype frequencies are computed from the allele frequencies 
estimated from the observed genotypes, there are some bounds on where the
points can go.  Those are indicated by dashed lines above.


### What about some published data sets?

I went through Molecular Ecology and Dryad and found about 12 data sets created by some
version of RAD (RAD, RAD-PE, ddRAD, etc.) that were in a format that was not
too hard to wrangle into an 012 matrix.  For each data set, I focused on the 
single sampling location with the largest sample size and made similar plots for
them.  Here are two:

![](figures/lobster_big_pop.png)

![](figures/snails_big_pop.png)

Wow! Those are some data sets that are, in the aggregate, showing severe
departures from Hardy-Weinberg equilibrium.  

I think in one of them the authors
said that they filtered out loci that were significantly out of HWE.  So, it might
be that with small samples, and focusing only on a single marker at a time, it is 
hard to detect these distortions.  But they are quite evident when viewed like this.
The main point: there are far more homozygotes than you would expect under HWE.

### A Simple Model of Genotyping Errors

When I look at these pictures, I think, "Let's make a super simple model of genotyping
error and estimate what the genotyping error rates must be to get things that look
this distorted."

Obviously, there are some heterozygotes that are being called as homozygotes. So,
let's propose this: all individuals that are truly homozygotes get correctly called
as homozygotes, but individuals that are truly heterozygous have a probability $m$ of
being miscalled as a homozygote.  If the individual is miscalled, then it has an equal
chance of being called as either homozygote. 

We can draw a DAG for this model, but let's establish a convention, first: rather than
naming alleles by the bases that they carry, we will just let the alleles be
labelled 0 and 1.  Then the possible genotypes, $G$ are denoted 0, 1, or 2, according to
the number of 1 alleles in the diploid genotype.

![](diagrams/geno-err-model.png)

$p_1$ is the frequency of the "1" allele.  

We don't have time to go over the details of the model, but they are straightforward
and trivial.  It takes just a few lines of R code to implement MCMC for it. It mixes
well, and even on large data sets it doesn't take too long to get an estimate of that
_heterozygote miscall rate_ parameter $m$.  

### A Gallery of Data sets

Here are all the data sets I investigated, ordered by $m$ from smallest to largest.

#### bonnethead_shark,  $m = 0.01$

![](figures/bonnethead_shark_big_pop.png)




#### wak_chinook,  $m = 0.02$

![](figures/wak_chinook_big_pop.png)


#### wifl,  $m = 0.03$

![](figures/wifl_big_pop.png)
Hey! Circle back to the bright spots on the boundaries.



#### red_drum,  $m = 0.05$

![](figures/red_drum_big_pop.png)




#### anguilla,  $m = 0.14$

![](figures/anguilla_big_pop.png)




#### chinook_hecht,  $m = 0.17$

![](figures/chinook_hecht_big_pop.png)




#### rostrata,  $m = 0.23$

![](figures/rostrata_big_pop.png)




#### lobster,  $m = 0.25$

![](figures/lobster_big_pop.png)




#### anchovy,  $m = 0.28$

![](figures/anchovy_big_pop.png)




#### snails,  $m = 0.45$

![](figures/snails_big_pop.png)





#### dolphin_L_albirostris,  $m = 0.65$

![](figures/dolphin_L_albirostris_big_pop.png)




#### dolphin_L_acutus,  $m = 0.72$

![](figures/dolphin_L_acutus_big_pop.png)


### Why is this?

Could be many issues (undermerged stacks for example), but in many cases
the heterozygote miscall is related to read depth---it is worse at low read
depths.  We can infer this with an expanded model:

![](slide_grabs/whoa-dag-with-rd.png)
And if we look at the results for lobster we see:

![](slide_grabs/lobster-whoa-with-read-depths.png)

This suggests that it has to do with the genotype calling itself.


### Loci on the boundaries --- these are likely paralogous loci

![](figures/whoa-wifl-fixed-paralogs.svg)

![](figures/whoa-wifl-fixed-paralogs-fixed-alt.svg)
![](figures/whoa-wifl-fixed-paralogs-fixed-ref.svg)

Note that the presence of such loci will artificially drive the
estimated heterozygote miscall rate _down_.




### Going forward from here

We just saw evidence of widespread genotyping error in some RAD datasets.  What should
be clear is that not all RAD data sets have inaccurate genotype calls.  But some of them
have really inaccurate genotype calls.  


Many of the problems observed in these data sets can be reduced by using an approach that
combines the genotype likelihoods with allele frequency information, rather than just
calling the highest-likelihood genotype in every situation.

### A low heterozygote miscall rate does not mean that there aren't many genotyping errors...

`whoa` is a great way to see if there are serious problems in a data set.  But, using it,
you cannot conclude that heterozygotes are accurately identified/called in a data set.

If the heterozygote miscall rate is low, it means that _on average_ you have the right
number of heterozygotes and homozygotes in a data set (or, perhaps far too many heterozygotes).

However, the genotyping accuracy might be quite low.  

### How much of a problem is it if your heterozygotes are often called as homozygotes?

This depends on what question you are addressing:

* _Allele frequency estimation_: not too much of a problem, but it could (proportionally) elevate the frequency of the rare allele a bit more.
* _Relationship inference_: absolutely disastrous.  Any time you are comparing genotypes between individuals to estimate relationships, genotyping error can have a big impact and must be accounted for.
* _Identification of $F_\mathrm{ST}$ outliers_: potentially problematic.
* etc.

### If inference can be made without calling genotypes, that is often quite useful for low-read depth sampling.


## Hands-on practical with `whoa`

1. Log on to the RStudio server.
2. At the R console do:

```r
library(whoa)
```

- Then, in the RStudio Help panel (lower right), type `whoa` in the text search box and hit return.
- Click the "Index" link at the bottom of the result.
- The click the "User guides, package vignettes,..." link.
- Choose the whoa tutorial and run through it.

### Further whoa fun

In the RStudio terminal from your home directory, do this:

```
cp -r instructor_materials/Eric_Anderson/whoa-practical-session ./
```
Then open the RStudio project inside there and follow the directions in: `001-whoa-first-steps.Rmd`.

If you have your own data in a VCF file, this shows how to read data
in from a VCF file and use it in a `whoa` analysis.


## Wrap Up

Key Take Home Messages:

1. To the Bayesian, probability is how we measure uncertainty.  
2. The posterior is proportional to the prior times the likelihood.
3. Monte Carlo sampling from a posterior distribution makes it very easy to assess the
posterior distribution of any function of the samples.
4. MCMC is useful to Bayesians because it can simulate from the posterior without needing
to compute the normalizing constant.
5. MCMC is great, but should be used with care.
6. In their essence, models for calling genotypes probabilistically while estimating and
accounting for allele frequencies are quite simple, and make a lot of sense.
7. There are a lot of RAD/GBS-derived data sets of genotypes that show evidence
of profoundly high rates of heterozygotes being miscalled as homozygotes. It doesn't seem
out of the question that such error rates could influence downstream inferences.
8. However, not all studies show very high genotyping error rates---high read depths and
intelligent/stringent filtering can prevail.
9. Probabilistic genotype calling is certainly a principled way to address these issues,
but it is not a panacea.  It still adheres to the principles of GIGO and can't make something
from nothing. 
10. Don't fool yourself into thinking that by using probabilistic genotype calling
you won't lose anything when you shave sequencing effort down to as little as possible.  
It is possible to
lose a fair bit when you go down to low read depth / coverage.  But, it depends on what
question you are trying to answer.













